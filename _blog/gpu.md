---
title: "GPU human docs"
collection: blog
type: "blog"
permalink: /gpu-human-docs/
date: 2025-07-21
excerpt: 'A collection of GPU-related human documentation.'
location: "Shanghi, China"
---

## GPU Human Documentation

This is a collection of human-readable documentation related to GPU (Graphics Processing Unit) technologies, including tutorials, guides, and explanations of various concepts and practices in the field of GPU computing.

## Contents

- [GPU Glossary](https://modal.com/gpu-glossary)
- [GPU Mode](https://www.youtube.com/@GPUMODE/videos)
- [Outperforming cuBLAS on H100: a Worklog](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog)
- [cuda mmm](https://siboehm.com/articles/22/CUDA-MMM)

## training

- [data parallelism](https://siboehm.com/articles/22/data-parallel-training)
- [pipeline parallelism](https://siboehm.com/articles/22/pipeline-parallel-training)
- [baidu ring allreduce](https://siboehm.com/articles/22/data-parallel-training)
- [Allreduce - the basis of multi-device communication for neural network training](https://marek.ai/allreduce-the-basis-of-multi-device-communication-for-neural-network-training.html)
- [pytorch distributed](https://arxiv.org/abs/2006.15704)
- [mpi docs](https://www.mpi-forum.org/docs/)
- [shallow speed](https://github.com/siboehm/shallowspeed)
- [The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed)
- [How to Train Really Large Models on Many GPUs?](https://lilianweng.github.io/posts/2021-09-25-train-large/)